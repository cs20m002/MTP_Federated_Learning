{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e25de4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f30f66db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "323420a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# training dataset preparation\n",
    "\n",
    "path = \"/home/fedlearn/Downloads/cifar-10-python/cifar-10-batches-py/\"\n",
    "os.chdir(path)\n",
    "\n",
    "trainX = []\n",
    "trainy = []\n",
    "\n",
    "for file in os.listdir():\n",
    "    if(\"data_batch\" in file):\n",
    "        file_name = path+file\n",
    "        data_batch = unpickle(file_name)\n",
    "        \n",
    "#         print(type(data_batch))\n",
    "#         print(data_batch.keys())\n",
    "        \n",
    "        data_X = data_batch[b'data']\n",
    "        data_X = data_X.reshape(len(data_X),3,32,32).transpose(0,2,3,1)\n",
    "        trainX.append(data_X)\n",
    "        \n",
    "        data_y = data_batch[b'labels']\n",
    "        data_y = np.array(data_y)\n",
    "        trainy.append(data_y.reshape(10000,1))\n",
    "\n",
    "train_images = np.concatenate((trainX[0],trainX[1],trainX[2],trainX[3],trainX[4]),axis=0)\n",
    "train_labels = np.concatenate((trainy[0],trainy[1],trainy[2],trainy[3],trainy[4]),axis=0)\n",
    "\n",
    "print(train_images.shape, train_labels.shape)\n",
    "print(type(train_images), type(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c056de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (10000, 1)\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# testing dataset preparation\n",
    "\n",
    "file_name = \"/home/fedlearn/Downloads/cifar-10-python/cifar-10-batches-py/test_batch\"\n",
    "\n",
    "data_batch = unpickle(file_name)\n",
    "\n",
    "# print(type(data_batch))\n",
    "# print(data_batch.keys())\n",
    "\n",
    "data_X = data_batch[b'data']\n",
    "data_X = data_X.reshape(len(data_X),3,32,32).transpose(0,2,3,1)\n",
    "\n",
    "data_y = data_batch[b'labels']\n",
    "data_y = np.array(data_y).reshape(10000,1)\n",
    "\n",
    "test_images, test_labels =  data_X, data_y\n",
    "\n",
    "print(test_images.shape, test_labels.shape)\n",
    "print(type(test_images), type(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84f33232",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Scaling\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c3903915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db725ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = getModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe43573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d140ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iid_dataset(trainX, trainY):\n",
    "  client_count=100\n",
    "  e0 = 500\n",
    "  s0 = 0\n",
    " \n",
    "  local_xy_iid=[]\n",
    "  for i in range(0,client_count):\n",
    "    local_x= trainX[s0:e0,:]\n",
    "    local_y= trainY[s0:e0,:]\n",
    "\n",
    "    local_xy_iid.append((local_x,local_y))\n",
    "    s0 = e0\n",
    "    e0 += 500\n",
    "\n",
    "  print(\"iid distribution of data done\")\n",
    "  return local_xy_iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07c489fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iid distribution of data done\n"
     ]
    }
   ],
   "source": [
    "local_xy_iid = get_iid_dataset(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74e69dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 32, 32, 3) (500, 1)\n"
     ]
    }
   ],
   "source": [
    "print(local_xy_iid[0][0].shape,local_xy_iid[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b239d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b094478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # finding optimal number of cluster in the local data\n",
    "# # The number of clusters for each of the dataset in not known (unsupervised)\n",
    "\n",
    "def minSilhouetteValue(local_X, local_y, count):\n",
    "  gmm_list = []\n",
    "  silhouette_val = []\n",
    "  for n_comp in range(2,8): # n_comp [2,3,4,5,6,7]\n",
    "    gmm = GaussianMixture(n_components=n_comp, random_state=10)\n",
    "    cluster_labels = gmm.fit_predict(local_X)\n",
    "    silhouette_val.append(silhouette_score(local_X, cluster_labels))\n",
    "    gmm_list.append(gmm)\n",
    "  \n",
    "  min_val_index = silhouette_val.index(min(silhouette_val))\n",
    "  print(\"Local Client \", count , \" : \" ,gmm_list[min_val_index])\n",
    "  return gmm_list[min_val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ee419ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhou = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "def9e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "temp = silhou\n",
    "\n",
    "with open('res.txt') as f:\n",
    "    lines = f.readlines()\n",
    "    for i in range(0,len(lines)):\n",
    "        ind = lines[i].find(\"s=\")\n",
    "        mini_sil = lines[i][ind+2:ind+3]\n",
    "        silhou.append(mini_sil)\n",
    "\n",
    "print(len(silhou))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c186ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since for 100 clients, in one shot kernel is now working\n",
    "# So, whaterver results comes we will store\n",
    "# And start again from the index where it ends\n",
    "\n",
    "# num = local_xy_iid[0][0].shape[0]\n",
    "# d1 = local_xy_iid[0][0].shape[1]\n",
    "# d2 = local_xy_iid[0][0].shape[2]\n",
    "# d3 = local_xy_iid[0][0].shape[3]\n",
    "\n",
    "# for i in range(0,100):\n",
    "#     X = local_xy_iid[i][0].reshape(num,d1*d2*d3)\n",
    "#     min_sil = minSilhouetteValue(X,local_xy_iid[i][1], i )\n",
    "#     silhou.append(min_sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7627c23f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# num = local_xy_iid[0][0].shape[0]\n",
    "# d1 = local_xy_iid[0][0].shape[1]\n",
    "# d2 = local_xy_iid[0][0].shape[2]\n",
    "# d3 = local_xy_iid[0][0].shape[3]\n",
    "\n",
    "# for i in range(33,100):\n",
    "#     X = local_xy_iid[i][0].reshape(num,d1*d2*d3)\n",
    "#     min_sil = minSilhouetteValue(X,local_xy_iid[i][1], i )\n",
    "# #     silhou.append(min_sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5775e348",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_list = []\n",
    "\n",
    "for i in range(0,31):\n",
    "    \n",
    "#     Finding best number of cluster using Silhouette Method\n",
    "#     X = local_xy_iid[i][0].reshape(num,d1*d2*d3)\n",
    "#     min_sil = minSilhouetteValue(X,local_xy_iid[i][1], i )\n",
    "    \n",
    "    # number of component for ith client\n",
    "    n_comp = int(silhou[i])\n",
    "    gmm_i = GaussianMixture(n_components=n_comp)\n",
    "    gmm_i.fit(X = local_xy_iid[i][0].reshape(-1,3072))\n",
    "    print(\"GMM : \", i ,\" Build\")\n",
    "    gmm_list.append(gmm_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b4d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(31,61):\n",
    "    \n",
    "# #     Finding best number of cluster using Silhouette Method\n",
    "# #     X = local_xy_iid[i][0].reshape(num,d1*d2*d3)\n",
    "# #     min_sil = minSilhouetteValue(X,local_xy_iid[i][1], i )\n",
    "    \n",
    "#     # number of component for ith client\n",
    "#     n_comp = int(silhou[i])\n",
    "#     gmm_i = GaussianMixture(n_components=n_comp)\n",
    "#     gmm_i.fit(X = local_xy_iid[i][0].reshape(-1,3072))\n",
    "#     print(\"GMM : \", i ,\" Build\")\n",
    "#     gmm_list.append(gmm_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94ce887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gmm_list_i = []\n",
    "\n",
    "# for i in range(40,100):\n",
    "    \n",
    "# #     Finding best number of cluster using Silhouette Method\n",
    "# #     X = local_xy_iid[i][0].reshape(num,d1*d2*d3)\n",
    "# #     min_sil = minSilhouetteValue(X,local_xy_iid[i][1], i )\n",
    "    \n",
    "#     # number of component for ith client\n",
    "#     n_comp = int(silhou[i])\n",
    "#     gmm_i = GaussianMixture(n_components=n_comp)\n",
    "#     gmm_i.fit(X = local_xy_iid[i][0].reshape(-1,3072))\n",
    "#     print(\"GMM : \", i ,\" Build\")\n",
    "#     gmm_list.append(gmm_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba7db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send local GMMs to Server & Do sampling at Server\n",
    "\n",
    "def syntheticDataGen(gmm_list):\n",
    "  n_samp = 500 # hyperparameter \n",
    "\n",
    "  syn_X_list = []\n",
    "  syn_y_list = []\n",
    "\n",
    "  for i in range(30):\n",
    "    gmm = gmm_list[i]\n",
    "    syn_X1,_ = gmm.sample(n_samp)\n",
    "    syn_y1 = np.full(n_samp,i)\n",
    "    print (syn_X1.shape,syn_y1.shape)\n",
    "\n",
    "    syn_X_list.append(syn_X1)\n",
    "    syn_y_list.append(syn_y1)\n",
    "\n",
    "  syn_X = np.concatenate(syn_X_list)\n",
    "  syn_y = np.concatenate(syn_y_list)\n",
    "  syn_y = to_categorical(syn_y)\n",
    "  print (syn_X, syn_y)\n",
    "  return syn_X, syn_y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c919f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic/proxy data generation\n",
    "syn_X1, syn_y1 = syntheticDataGen(gmm_list)\n",
    "print(syn_X1.shape, X_central_train.shape)\n",
    "print(syn_X1.shape, X_central_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea8f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateCentralModelGMM(syn_X, syn_y):\n",
    "  updated_model = define_model()\n",
    "  updated_model.compile(optimizer=SGD(learning_rate=0.01, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  print (syn_X.shape,syn_y.shape)\n",
    "\n",
    "  updated_model.fit(syn_X, syn_y, epochs=100, batch_size=500, validation_data=(testX, testY))\n",
    "\n",
    "  _, accuracy = updated_model.evaluate(testX, testY,verbose=0) # returns loss and accuracy\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_acc_gmm = updateCentralModelGMM(syn_X1, syn_y1)\n",
    "print(new_acc_gmm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
