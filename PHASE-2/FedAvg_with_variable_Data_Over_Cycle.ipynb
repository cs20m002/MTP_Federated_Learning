{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XU8giWU-I5C_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "from keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRgs66RUpnLk"
      },
      "source": [
        "##Loading Data\n",
        "\n",
        "Each datapoint is a 32x32 pixel color photographs\n",
        "\n",
        "Classes : 10\n",
        "\n",
        "[0: airplane]\n",
        "[1: automobile]\n",
        "[2: bird]\n",
        "[3: cat]\n",
        "[4: deer]\n",
        "[5: dog]\n",
        "[6: frog]\n",
        "[7: horse]\n",
        "[8: ship]\n",
        "[9: truck]\n",
        "\n",
        "Samples per class ~6000\n",
        "\n",
        "Samples total : 60000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMWPmrn6LcKh"
      },
      "outputs": [],
      "source": [
        "#Scale Pixels\n",
        "def prep_pixels(trainX, testX):\n",
        "\t# convert from integers to floats\n",
        "\ttrain_norm = trainX.astype('float32')\n",
        "\ttest_norm = testX.astype('float32')\n",
        " \n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "\n",
        "# load train and test dataset\n",
        "def load_dataset():\n",
        "\t# load dataset\n",
        "\t(trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        " \n",
        "  # summarize loaded dataset\n",
        "\tprint('Train: X=%s, y=%s' % (trainX.shape, trainY.shape))\n",
        "\tprint('Test: X=%s, y=%s' % (testX.shape, testY.shape))\n",
        "\t\n",
        "\t# scale pixels\n",
        "\ttrainX, testX = prep_pixels(trainX, testX)\n",
        " \n",
        "\treturn trainX, trainY, testX, testY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9Cmpl4ypz8y"
      },
      "source": [
        "##Shuffling Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wPwzCP_nWQo"
      },
      "outputs": [],
      "source": [
        "def shuffle_dataset(trainX,trainY,testX,testY):\n",
        "  # number of datapoints [0,1,2,.....,n]\n",
        "  no_dataPoints_train = np.arange(0,trainX.shape[0]) # 50000\n",
        "  no_dataPoints_test = np.arange(0,testX.shape[0])   # 10000\n",
        "\n",
        "  # random shuffling\n",
        "  np.random.shuffle(no_dataPoints_train)\n",
        "  np.random.shuffle(no_dataPoints_test)\n",
        "\n",
        "  # reshuffled original train data\n",
        "  X_train = trainX[no_dataPoints_train]\n",
        "  y_train = trainY[no_dataPoints_train]\n",
        "\n",
        "  # reshuffled original test data\n",
        "  X_test = testX[no_dataPoints_test]\n",
        "  y_test = testY[no_dataPoints_test]\n",
        "\n",
        "  return X_train,y_train,X_test,y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading dataset and Storing label specific index\n"
      ],
      "metadata": {
        "id": "79DhRkGpRObG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg7GNb94qyrw",
        "outputId": "8cd76a77-f540-4f87-c1e5-ad9cc9f5fee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "Train: X=(50000, 32, 32, 3), y=(50000, 1)\n",
            "Test: X=(10000, 32, 32, 3), y=(10000, 1)\n"
          ]
        }
      ],
      "source": [
        "trainX, trainY, testX, testY = load_dataset()\n",
        "trainX, trainY, testX, testY = shuffle_dataset(trainX, trainY, testX, testY)\n",
        "\n",
        "# each class datapoints index\n",
        "each_Cls_DataIndex_train = []\n",
        "\n",
        "for i in range(10) :\n",
        "  _, class_i_train = np.where([trainY[:,0]==i]) # trainY = [[1],[2],[3]....]\n",
        "  each_Cls_DataIndex_train.append(class_i_train)\n",
        "  \n",
        "# one hot encode target values\n",
        "trainY = to_categorical(trainY)\n",
        "testY = to_categorical(testY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEHB0Eyoqurg"
      },
      "source": [
        "##Central Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n67-C9Smz0yZ",
        "outputId": "08683b48-cb7b-4842-daef-7c6d6de8cd7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of dataPoints from each class for training :  1500\n",
            "Number of dataPoints for training :  15000\n"
          ]
        }
      ],
      "source": [
        "# 30% data samples to central \n",
        "n_pts_training = int((trainX.shape[0]*.3)/10)\n",
        "\n",
        "print(\"Number of dataPoints from each class for training : \",n_pts_training)\n",
        "\n",
        "print(\"Number of dataPoints for training : \",n_pts_training*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIzlt3Un00Ly",
        "outputId": "662293d1-22e0-42c1-c1c5-b2a1d3ec2950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Central Model dataset\n",
            "X_train :  (15000, 32, 32, 3) y_train :  (15000, 10)\n",
            "X_validate :  (10000, 32, 32, 3) y_validate :  (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "central_dataPoint_index_train = []\n",
        "\n",
        "for i in range(10) :\n",
        "  ind1 = each_Cls_DataIndex_train[i][0:n_pts_training]\n",
        "  central_dataPoint_index_train.extend(ind1)  # extend so that 1D [ind[0], ind[1],...] array not like this [[ind]]\n",
        "\n",
        "X_central_train = trainX[central_dataPoint_index_train]\n",
        "y_central_train = trainY[central_dataPoint_index_train]\n",
        "\n",
        "\n",
        "print(\"Central Model dataset\")\n",
        "print(\"X_train : \", X_central_train.shape, \"y_train : \", y_central_train.shape)\n",
        "print(\"X_validate : \", testX.shape, \"y_validate : \", testY.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evkRJxhGpZpc"
      },
      "source": [
        "##Central Model ( Basic Arch )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QqFCZ35iPBf"
      },
      "outputs": [],
      "source": [
        "# define cnn model\n",
        "def define_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
        "  model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiWJsGxXeJuv",
        "outputId": "afbe801d-cf15-4228-e560-f484b3ec7e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 4, 4, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 550,570\n",
            "Trainable params: 550,570\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = define_model()\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPv6We1t5Dcx"
      },
      "outputs": [],
      "source": [
        "# compile model\n",
        "model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOLbZrvW5d6l",
        "outputId": "17b80053-388a-4d5b-a23b-4b74915de402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "150/150 [==============================] - 16s 39ms/step - loss: 2.0823 - accuracy: 0.2432 - val_loss: 1.9006 - val_accuracy: 0.3206\n",
            "Epoch 2/20\n",
            "150/150 [==============================] - 6s 42ms/step - loss: 1.7948 - accuracy: 0.3629 - val_loss: 1.7059 - val_accuracy: 0.3855\n",
            "Epoch 3/20\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 1.6716 - accuracy: 0.4029 - val_loss: 1.6184 - val_accuracy: 0.4205\n",
            "Epoch 4/20\n",
            "150/150 [==============================] - 5s 35ms/step - loss: 1.5643 - accuracy: 0.4365 - val_loss: 1.5046 - val_accuracy: 0.4609\n",
            "Epoch 5/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.4689 - accuracy: 0.4742 - val_loss: 1.4596 - val_accuracy: 0.4750\n",
            "Epoch 6/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 1.4175 - accuracy: 0.4917 - val_loss: 1.3904 - val_accuracy: 0.5044\n",
            "Epoch 7/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.3441 - accuracy: 0.5207 - val_loss: 1.3733 - val_accuracy: 0.5050\n",
            "Epoch 8/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.2841 - accuracy: 0.5429 - val_loss: 1.3636 - val_accuracy: 0.5075\n",
            "Epoch 9/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.2301 - accuracy: 0.5672 - val_loss: 1.3341 - val_accuracy: 0.5219\n",
            "Epoch 10/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.1690 - accuracy: 0.5922 - val_loss: 1.2519 - val_accuracy: 0.5540\n",
            "Epoch 11/20\n",
            "150/150 [==============================] - 5s 34ms/step - loss: 1.1298 - accuracy: 0.6023 - val_loss: 1.2447 - val_accuracy: 0.5523\n",
            "Epoch 12/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 1.0876 - accuracy: 0.6207 - val_loss: 1.2371 - val_accuracy: 0.5647\n",
            "Epoch 13/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 1.0414 - accuracy: 0.6343 - val_loss: 1.2656 - val_accuracy: 0.5498\n",
            "Epoch 14/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 1.0035 - accuracy: 0.6501 - val_loss: 1.2217 - val_accuracy: 0.5704\n",
            "Epoch 15/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.9342 - accuracy: 0.6739 - val_loss: 1.2937 - val_accuracy: 0.5501\n",
            "Epoch 16/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.8943 - accuracy: 0.6921 - val_loss: 1.2037 - val_accuracy: 0.5854\n",
            "Epoch 17/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.8465 - accuracy: 0.7087 - val_loss: 1.2860 - val_accuracy: 0.5687\n",
            "Epoch 18/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.8046 - accuracy: 0.7237 - val_loss: 1.2320 - val_accuracy: 0.5799\n",
            "Epoch 19/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.7496 - accuracy: 0.7424 - val_loss: 1.2361 - val_accuracy: 0.5834\n",
            "Epoch 20/20\n",
            "150/150 [==============================] - 5s 36ms/step - loss: 0.7056 - accuracy: 0.7603 - val_loss: 1.3289 - val_accuracy: 0.5633\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f359050d810>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.fit(X_central_train, y_central_train, epochs=20, batch_size=100, validation_data=(testX, testY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq5e3JzGmStl"
      },
      "outputs": [],
      "source": [
        "weights_central = model.get_weights()\n",
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_central_Avg = []\n",
        "accuracy_central_GMM = []\n",
        "\n",
        "_, accuracy = model.evaluate(testX, testY,verbose=0) # returns loss and accuracy\n",
        "accuracy_central_Avg.append(round(accuracy,2))\n",
        "accuracy_central_GMM.append(round(accuracy,2))\n"
      ],
      "metadata": {
        "id": "12Cur6K6Qk2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXckY0FQi2cM"
      },
      "source": [
        "##Data Preparation for Local Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vw-o5ManEeow"
      },
      "source": [
        "###Per local client ==> 3500 class specific point\n",
        "creating a mixture dataPoints to add to all this ==> 900 (90 from each class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qendAIHJngOc"
      },
      "outputs": [],
      "source": [
        "# making a mixture data\n",
        "\n",
        "mix_Xy_train = []\n",
        "\n",
        "for i in range(10):\n",
        "  ind1 = each_Cls_DataIndex_train[i][n_pts_training:n_pts_training+900] # 90 from 10 classes ==> 900\n",
        "  \n",
        "  X_mix_train = trainX[ind1]\n",
        "  y_mix_train = trainY[ind1]\n",
        "  \n",
        "  mix_Xy_train.append((X_mix_train,y_mix_train))\n",
        "\n",
        "n_pts_training += 900"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# without data change cycle "
      ],
      "metadata": {
        "id": "QKa6s80DXQgA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTk7CZjQjDSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce1e971-fa32-4fe3-deec-40c6cedba608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration :  0\n",
            "Local Clent  0\n",
            "Training data X y :  (2690, 32, 32, 3) (2690, 10)\n"
          ]
        }
      ],
      "source": [
        "# # Control variable \n",
        "# onlyOnce = True\n",
        "# onlyIter = True;\n",
        "\n",
        "\n",
        "# local_Xy_train_batch = []\n",
        "\n",
        "# # in each batch will mix 10 data points\n",
        "# index_mix_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "# for j in range(10):\n",
        "#   local_Xy_train = []\n",
        "\n",
        "#   for i in range(1) :\n",
        "#     ind1 = each_Cls_DataIndex_train[i][n_pts_training:n_pts_training+2600]\n",
        "  \n",
        "#     X_local_train = trainX[ind1]\n",
        "#     y_local_train = trainY[ind1]\n",
        "#     # print(len(X_local_train), len(y_local_train))\n",
        "    \n",
        "    \n",
        "#     for k in range(10):\n",
        "      \n",
        "#       if(k!=i):\n",
        "#         X_cat_b_t = np.concatenate((X_local_train , mix_Xy_train[k][0][index_mix_train[k]:index_mix_train[k]+10]),axis=0)\n",
        "#         y_cat_b_t = np.concatenate((y_local_train , mix_Xy_train[k][1][index_mix_train[k]:index_mix_train[k]+10]),axis=0)\n",
        "        \n",
        "#         index_mix_train[k] += 10;\n",
        "\n",
        "#         X_local_train = X_cat_b_t\n",
        "#         y_local_train = y_cat_b_t\n",
        " \n",
        "      \n",
        "#     if(onlyOnce):\n",
        "#       if(onlyIter):\n",
        "#         print(\"Iteration : \" , j)\n",
        "#         onlyIter = False;\n",
        "#       print(\"Local Clent \", i);\n",
        "#       print(\"Training data X y : \", X_local_train.shape, y_local_train.shape)\n",
        "    \n",
        "#     local_Xy_train.append((X_cat_b_t,y_cat_b_t))\n",
        "    \n",
        "#   onlyOnce = False\n",
        "#   local_Xy_train_batch.append(local_Xy_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#With data change"
      ],
      "metadata": {
        "id": "aE94qOXVXYDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Control variable \n",
        "onlyOnce = True\n",
        "onlyIter = True;\n",
        "\n",
        "\n",
        "local_Xy_train_batch = []\n",
        "\n",
        "# in each batch will mix 10 data points\n",
        "index_mix_train = [0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "for j in range(10):\n",
        "  local_Xy_train = []\n",
        "\n",
        "  for i in range(10) :\n",
        "    ind1 = each_Cls_DataIndex_train[i][n_pts_training:n_pts_training+260]\n",
        "  \n",
        "    X_local_train = trainX[ind1]\n",
        "    y_local_train = trainY[ind1]\n",
        "    # print(len(X_local_train), len(y_local_train))\n",
        "    \n",
        "    \n",
        "    for k in range(10):\n",
        "      \n",
        "      if(k!=i):\n",
        "        X_cat_b_t = np.concatenate((X_local_train , mix_Xy_train[k][0][index_mix_train[k]:index_mix_train[k]+90]),axis=0)\n",
        "        y_cat_b_t = np.concatenate((y_local_train , mix_Xy_train[k][1][index_mix_train[k]:index_mix_train[k]+90]),axis=0)\n",
        "        \n",
        "        index_mix_train[k] += 90;\n",
        "\n",
        "        X_local_train = X_cat_b_t\n",
        "        y_local_train = y_cat_b_t\n",
        " \n",
        "      \n",
        "    if(onlyOnce):\n",
        "      if(onlyIter):\n",
        "        print(\"Iteration : \" , j)\n",
        "        onlyIter = False;\n",
        "      print(\"Local Clent \", i);\n",
        "      print(\"Training data X y : \", X_local_train.shape, y_local_train.shape)\n",
        "    \n",
        "    local_Xy_train.append((X_cat_b_t,y_cat_b_t))\n",
        "    \n",
        "  onlyOnce = False\n",
        "  local_Xy_train_batch.append(local_Xy_train)\n",
        "  n_pts_training += 260"
      ],
      "metadata": {
        "id": "MzuZ-qQxmtTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym1zRK07NFpr"
      },
      "source": [
        "##averaging of weights at central"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCex8WXTl_gN"
      },
      "outputs": [],
      "source": [
        "# weights_array = [[0],[1],[2],.....[9]]\n",
        "# weights_array[i] = [[0],[1],[2],.....[15]]\n",
        "# weights_array[i][j] is a np array\n",
        "\n",
        "def federated_averaging(weights_array):\n",
        "  mean_weights = []\n",
        "\n",
        "  for i in range(len(weights_array[0])):    # i = [0,..,15]\n",
        "    flag = True\n",
        "    for j in range(len(weights_array)):     # j = [0,..,9]\n",
        "      if(flag):\n",
        "        temp = np.full(weights_array[j][i].shape, 0, dtype=np.float32) # temp full of 0(zeros)\n",
        "        temp = weights_array[j][i] # this step is important bcoz 0+x/2 will result in error\n",
        "        flag = False\n",
        "      \n",
        "      temp = np.mean((temp,weights_array[j][i]), axis=0) \n",
        "    mean_weights.append(temp)\n",
        "\n",
        "  updated_model = define_model()\n",
        "  updated_model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  updated_model.set_weights(mean_weights)\n",
        "  _, accuracy = updated_model.evaluate(testX, testY,verbose=0) # returns loss and accuracy\n",
        "  return mean_weights, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GMM"
      ],
      "metadata": {
        "id": "uoNJ6j9P48Fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.mixture import GaussianMixture\n",
        "# from sklearn.metrics import silhouette_samples, silhouette_score"
      ],
      "metadata": {
        "id": "nnHgUl_R5AyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # finding optimal number of cluster in the local data\n",
        "# # The number of clusters for each of the dataset in not known (unsupervised)\n",
        "\n",
        "def minSilhouetteValue(local_X, local_y):\n",
        "  gmm_list = []\n",
        "  silhouette_val = []\n",
        "  for n_comp in range(2,8): # n_comp [2,3,4,5,6,7]\n",
        "    gmm = GaussianMixture(n_components=n_comp, random_state=10)\n",
        "    cluster_labels = gmm.fit_predict(local_X)\n",
        "    silhouette_val.append(silhouette_score(local_X, cluster_labels))\n",
        "    gmm_list.append(gmm)\n",
        "  \n",
        "  min_val_index = silhouette_val.index(min(silhouette_val))\n",
        "  print(gmm_list[min_val_index])\n",
        "  return gmm_list[min_val_index]\n"
      ],
      "metadata": {
        "id": "N6DSYOeE5bgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send local GMMs to Server & Do sampling at Server\n",
        "\n",
        "def syntheticDataGen(gmm_list):\n",
        "  n_samp = 20 # hyperparameter \n",
        "\n",
        "  syn_X_list = []\n",
        "  syn_y_list = []\n",
        "\n",
        "  for i in range(10):\n",
        "    gmm = gmm_list[i]\n",
        "    syn_X1,_ = gmm.sample(n_samp)\n",
        "    syn_y1 = np.full(n_samp,i)\n",
        "    print (syn_X1.shape,syn_y1.shape)\n",
        "\n",
        "    syn_X_list.append(syn_X1)\n",
        "    syn_y_list.append(syn_y1)\n",
        "\n",
        "  syn_X = np.concatenate(syn_X_list)\n",
        "  syn_y = np.concatenate(syn_y_list)\n",
        "  syn_y = to_categorical(syn_y)\n",
        "  print (syn_X, syn_y)\n",
        "  return syn_X, syn_y  "
      ],
      "metadata": {
        "id": "EqktfEiiL_zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def updateCentralModelGMM(syn_X, syn_y,X_central_train,y_central_train):\n",
        "#   updated_model = define_model()\n",
        "#   updated_model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#   # for i in range(len(syn_X)):\n",
        "#   X_central_train = np.concatenate((X_central_train,syn_X.reshape(200,32,32,3)),axis=0)\n",
        "  \n",
        "#   y_central_train = np.concatenate((y_central_train,syn_y),axis=0)\n",
        "#   print (X_central_train.shape,y_central_train.shape)\n",
        "\n",
        "#   updated_model.fit(X_central_train, y_central_train, epochs=20, batch_size=100, validation_data=(testX, testY))\n",
        "\n",
        "#   _, accuracy = updated_model.evaluate(testX, testY,verbose=0) # returns loss and accuracy\n",
        "\n",
        "#   return accuracy\n"
      ],
      "metadata": {
        "id": "PKrr09dgOIrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfhg8S-aLNTA"
      },
      "source": [
        "#Creating Local Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Iteration [1-10]"
      ],
      "metadata": {
        "id": "oqp1xjsyS5TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  print(\"Cycle : \", i)\n",
        "  weights_local = []\n",
        "\n",
        "  local_model = define_model()\n",
        "  local_model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  for j in range(1):\n",
        "    # local model for Jth local client\n",
        "    local_model.set_weights(weights_central)\n",
        "\n",
        "    # local dataset for cycle i & jth client\n",
        "    X_l_train, y_l_train = local_Xy_train_batch[i][j]\n",
        "\n",
        "    local_model.fit(X_l_train, y_l_train, epochs=10, batch_size=100, validation_data=(testX, testY),verbose=1)\n",
        "    localweights = local_model.get_weights()\n",
        "    weights_local.append(localweights)\n",
        "\n",
        "  # averaging of weights function call\n",
        "  mean_weights_each_cycle, new_acc_avg = federated_averaging(weights_local)\n",
        "  accuracy_central_Avg.append(new_acc_avg)\n",
        "  weights_central = mean_weights_each_cycle\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "EY0UgBbv-uK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(10):\n",
        "#   print(\"Cycle : \", i)\n",
        "#   gmm_list = []\n",
        "\n",
        "#   for j in range(10):\n",
        "\n",
        "#     # local dataset for cycle i & jth client\n",
        "#     X_l_train, y_l_train = local_Xy_train_batch[i][j]\n",
        "\n",
        "#     # n_comp = minSilhouetteValue(X_l_train.reshape(-1,1), y_l_train)\n",
        "#     # print(n_comp)\n",
        "\n",
        "#     gmm_i = GaussianMixture(n_components=3)\n",
        "#     gmm_i.fit(X = X_l_train.reshape(-1,3072))\n",
        "#     gmm_list.append(gmm_i)\n",
        "\n",
        "#   # synthetic/proxy data generation\n",
        "#   syn_X1, syn_y1 = syntheticDataGen(gmm_list)\n",
        "#   print(syn_X1.shape, X_central_train.shape)\n",
        "#   print(syn_X1.shape, X_central_train.shape)\n",
        "\n",
        "#   new_acc_gmm = updateCentralModelGMM(syn_X1, syn_y1, X_central_train, y_central_train)\n",
        "#   accuracy_central_GMM.append(new_acc_gmm)\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "P4SCzJQm_FsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869zkIsF58nK"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "\n",
        "  print(\"Cycle : \", i)\n",
        "  weights_local = []\n",
        "  gmm_list = []\n",
        "\n",
        "  local_model = define_model()\n",
        "  local_model.compile(optimizer=SGD(learning_rate=0.001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  for j in range(10):\n",
        "    # local model for Jth local client\n",
        "    local_model.set_weights(weights_central)\n",
        "\n",
        "    # local dataset for cycle i & jth client\n",
        "    X_l_train, y_l_train = local_Xy_train_batch[i][j]\n",
        "\n",
        "    local_model.fit(X_l_train, y_l_train, epochs=10, batch_size=100, validation_data=(testX, testY),verbose=1)\n",
        "    localweights = local_model.get_weights()\n",
        "    weights_local.append(localweights)\n",
        "\n",
        "    # n_comp = minSilhouetteValue(X_l_train.reshape(-1,1), y_l_train)\n",
        "    # print(n_comp)\n",
        "\n",
        "    gmm_i = GaussianMixture(n_components=3)\n",
        "    gmm_i.fit(X = X_l_train.reshape(-1,3072))\n",
        "    gmm_list.append(gmm_i)\n",
        "\n",
        "  # averaging of weights function call\n",
        "  mean_weights_each_cycle, new_acc_avg = federated_averaging(weights_local)\n",
        "  accuracy_central_Avg.append(new_acc_avg)\n",
        "  weights_central = mean_weights_each_cycle\n",
        "\n",
        "  # synthetic/proxy data generation\n",
        "  syn_X1, syn_y1 = syntheticDataGen(gmm_list)\n",
        "  print(syn_X1.shape, X_central_train.shape)\n",
        "  print(syn_X1.shape, X_central_train.shape)\n",
        "\n",
        "  new_acc_gmm = updateCentralModelGMM(syn_X1, syn_y1, X_central_train, y_central_train)\n",
        "  accuracy_central_GMM.append(new_acc_gmm)\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwCMUzT0O4VA"
      },
      "outputs": [],
      "source": [
        "from prettytable import PrettyTable \n",
        "\n",
        "def printAccuracy(accuracy):\n",
        "  accTable1 = PrettyTable([\"Cycle \", \"Accuracy\"])\n",
        "\n",
        "  for i in range(10):\n",
        "    accTable1.add_row([str(i),str(accuracy[i])])\n",
        "  print('\\n')\n",
        "  print(accTable1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print Accuracy\n",
        "print(\"Averaging\")\n",
        "printAccuracy(accuracy_central_Avg)\n",
        "\n",
        "# print(\"GMM\")\n",
        "# printAccuracy(accuracy_central_GMM)"
      ],
      "metadata": {
        "id": "NygblFy4IKai",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6407da61-c596-4b7d-bc5d-3ad573fb6b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Averaging\n",
            "\n",
            "\n",
            "+--------+---------------------+\n",
            "| Cycle  |       Accuracy      |\n",
            "+--------+---------------------+\n",
            "|   0    |         0.56        |\n",
            "|   1    | 0.26179999113082886 |\n",
            "|   2    | 0.33959999680519104 |\n",
            "|   3    |  0.3301999866962433 |\n",
            "|   4    |  0.3695000112056732 |\n",
            "|   5    | 0.34279999136924744 |\n",
            "|   6    |  0.335999995470047  |\n",
            "|   7    | 0.37220001220703125 |\n",
            "|   8    | 0.37619999051094055 |\n",
            "|   9    | 0.35179999470710754 |\n",
            "+--------+---------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FedAvg with variable Data Over Cycle.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}